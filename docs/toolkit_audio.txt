# Migrating Obi to toolkit asset management

First Draft by Avneesh on May 31, 2007. Edited by Julien June 1st, 2007.

Also contains features required by the Application team in the toolkit asset manager.

<div id="toc">Table of contents</div>

## Analysis of the toolkit asset management

The toolkit contains two implementations of audio assets:

  1. `ExternalAudioMedia`
  2. `AudioMediaData`

`ExternalAudioMedia` deals with direct access to physical files while
`AudioMediaData` deals with virtual concept similar to Obi's native
`AudioMediaAsset` and `AssetManager`.

### `ExternalAudioMedia`
[`ExternalAudioMedia`](http://urakawa.sourceforge.net/sdk/apidoc/org/daisy/urakawa/media/ExternalAudioMedia.html) represents a part of an audio file being represented in
`ExternalAudioMedia` with its `src`, `BeginTime` and `EndTime`. This maps
easily to an [`audio`
element](http://www.w3.org/TR/2005/REC-SMIL2-20051213/extended-media-object.html#edef-ref) in SMIL; and is similar to Obi's use of _audio
clips_ for virtual editing.

### `AudioMediaData` and `WavAudioMediaData`
[`AudioMediaData`](http://urakawa.sourceforge.net/sdk/apidoc/org/daisy/urakawa/media/data/AudioMediaData.html)
is similar to Obi's native `AudioMediaAsset` class.
`AudioMediaData` is abstract and implemented in `WavAudioMediaData` for raw RIFF/WAV format.

[`WavAudioMediaData`](http://urakawa.sourceforge.net/sdk/apidoc/org/daisy/urakawa/media/data/codec/audio/WavAudioMediaData.html)
mainly contains a reference to a manager with 
another protected class named `WavClip` (note: this seems to be
implementation-dependent, as this is not part of the model itself)
which basically represents a part of physical wave file with the following members:

* clip start  time w.r.t. file.
* clip end time w.r.t. file.
* [`IDataProvider`](http://urakawa.sourceforge.net/sdk/apidoc/org/daisy/urakawa/media/data/DataProvider.html) implemented by [`FileDataProvider`](http://urakawa.sourceforge.net/sdk/apidoc/org/daisy/urakawa/media/data/FileDataProvider.html) which further contains the src information of an audio file on disk and internet media type information.

The `WavAudioMediaData` class contains a list of `WavClip`s
`List<WavClip>` arranged in playback order. It provides the following
functions for audio editing:

* `Stream getAudioData(ITime clipBegin, ITime clipEnd)`
* `void appendAudioData(Stream pcmData, ITimeDelta duration)`
* `insertAudioData(Stream pcmData, ITime insertPoint, ITimeDelta duration)`
* `void replaceAudioData(Stream pcmData, ITime replacePoint, ITimeDelta
duration)`
* `void removeAudio(ITime clipBegin, ITime clipEnd)`
* `void delete()`

Audio editing is mainly stream-based in `WavAudioMediaData` which is good as
it do not break encapsulation when we need to dig out binary data from audio
files, and works well for low-level audio operations such as playback,
phrase detection or recording; but depending only on streams makes things
complex as many times during audio editing, we are dealing with
AudioMediaData objects and not with bytes from files, as in split or merge.

## Issues and proposed fixes

### Audio recorder

Existing code flow is as follows (using Obi's native asset types):

1. An `AudioMediaAsset` is passed to the start recording function of
`AudioRecorder`.
2. `AudioRecorder` creates a physical file with audio format matching the
format of the `AudioMediaAsset` received.
3. After recording has finished, the WAV file just created by
`AudioRecorder` is encapsulated in an `AudioClip` and appended to the clip
list of the instance of the audio media asset.

This doesn't work with the toolkit because of stricter encapsulation (which
is a good thing--we got it wrong in Obi.) The proposed solutions are:

1. `AudioMediaData` in the toolkit should have a function which receives an
audio file path or its equivalent as a parameter and returns a PCM stream,
such as `Stream getStreamFromFile(Uri src)`.
This PCM stream can be then used in the function listed above for including
the data newly recorded in the `AudioMediaData` instance that passed as
argument to the recorder. It is recommended to have this function as `static`
(or as a class method) such that it is not mandatory to create an instance of
`AudioMediaData` to get a stream out of a file.
2. Have functions to append, insert and replace audio data directly from
audio files, _i.e._ overloads for functions `appendAudioData()`,
`insertAudioData()` and `replaceAudioData()` with URI (for file location) as input parameter instead of stream.


### Importing audio files

Existing code flow is as follows:

1. External WAV file is encapsulated in an `AudioClip` with format
information obtained from the WAV file.
2. A new  `AudioMediaAsset` is created with a list of clips this single
`AudioClip`. The format of this `AudioMediaAsset` is obtained from the clip.

Again the encapsulation of the `WavClip` prevents us to do this. Proposed
solutions:

1. Have a function `AudioMediaData importFile(Uri src)` returning an
`AudioMediaData` object with audio data and format populated from the file
at the given URI (or any other location mean.)
2. Use the `getStreamFromFile()` function described above to get a stream from an external WAV file, and do following work in Obi:
  * create a new `AudioMediaData` with no audio;
  * use the stream created from the file as a parameter to the `appendAudio()` function listed above in order to fill this `AudioMediaData` object with audio. 


### Split and Merge

Both of these functions operate at the higher `AudioMediaAsset` layer in
Obi. It is therfore proposed to have functions in the toolkit
`AudioMediaData` to accomplish splitting and merging at the `AudioMediaData`
or `WavAudioMediaData` layer as follows:

1. `AudioMediaData split(AudioMediData ass, ITime splitTime)` splits the
given asset at the given split time by modifiying the end point of the
original asset and returning the second half as a new asset. (Note: two new
assets could be returned instead of modifying the first one in place.)
2. `AudioMediaData merge(AudioMediaData ass)` merges two assets and returns
their concatenation.

Remark: the returned `AudioMediaData` objects should not share clips with
the source `AudioMediaData`, but only physical files.

### Phrase Detection

Existing code flow is as follows:

* The underlying binary data from clips is processed to mark phrases w.r.t. time.
* The time information is used to create AudioMediaAssets.

The phrase detector will not be part of core toolkit as discussed earlier,
so it cannot use the underlying `WavClip` class. As a result, we need the
following function in the toolkit to create AudioMediaData according to time
markings:

1. `AudioMediaData getChunk(ITime beginTime, ITime endTime)` extracts audio
data from the the original asset between begin and end times.

Remark: another possible implementation is to use `split()` iteratively.


### PlayBack

Playback is mainly dependent on streams which are already provided by the
toolkit. However, in many situations only a selected part of audio (_e.g._
during splitting preview, or when waveform editing is implemented and a
chunk of audio is selected) needs to be played so Obi relies on the
`getChunk()` function described above for playback as well.

Remark: the chunks created for playback need not be managed, so we could add
a flag to `getChunk()` to control this.

Remark: can we clip streams?
